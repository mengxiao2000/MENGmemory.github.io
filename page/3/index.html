<!DOCTYPE html>
<html lang="zh">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme-color" content="#58b77a">
  <title>黎明已经不远</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Syiao">
  <meta name="keywords" content>
  <meta name="description" content="“多新鲜啊~~”">
  <script id="hexo-configurations">
  var CONFIG = {
    root: '/mengxiao2000.github.io/',
    theme: 'lx',
    version: '0.3.9',
    localsearch:{
      "enable": false,
      "trigger": "auto",
      "top_n_per_article": 1,
      "unescape": false,
      "preload": false
      },
    path: 'null'
  };
</script>

  <link rel="shortcut icon" href="/mengxiao2000.github.io/favicon.ico">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/theme-lx@0.3.9/source/css/main.min.css">
  <style type="text/css">
    pre,
    code {
      font-family: 'Fira Code', monospace;
    }
    html {
      font-family: sans-serif;
    }
    body {
      font-family: sans-serif;
    }
    h1, h2, h3, h4, h5, figure {
      font-family: sans-serif;
    }
    .menu-container{
      font-family: sans-serif;
    }
  </style>

  <script src="//cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/theme-lx@0.3.9/source/js/jquery.jside.menu.js"></script>
	<script>
	$(document).ready(function(){
	$(".menu-container").jSideMenu({
	    jSidePosition: "position-right",
	    jSideSticky: true,
	    jSideSkin: "greenish",
	     });
	}); 
	</script>
  <!--baidu_analytics-->
<script>
  var _hmt = _hmt || [];
  (function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?e191a1666c";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Fira Code:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4/css/font-awesome.min.css">
<link rel="alternate" href="/mengxiao2000.github.io/atom.xml" title="黎明已经不远" type="application/atom+xml">
</head>
<body>
<div id="page">

<div id="lx-aside" style="background-image: url(//cdn.jsdelivr.net/npm/theme-lx@0.3.9/source/images/cover.jpeg)">
  <div class="overlay">
  <div class="featured">
    <div class="avatar"><a href="/mengxiao2000.github.io/"><img src="/mengxiao2000.github.io/images/avatar.jpeg"></a></div>
    <span>Syiao</span>
    <h1>黎明已经不远</h1>
    <span>——洗洗睡吧</span>
    
  </div>
  </div>
</div>

<div id="lx-main-content">
  <div class="lx-post">

  <div class="lx-entry padding">
    <div>
      <h2 class="title"><a href="/mengxiao2000.github.io/2019/11/06/E173C137-23FA-40C0-88AE-1E19BD761AB2/">运行TensorFlow</a></h2>
      <span class="lx-post-detail"><i class="fa fa-calendar-o"></i> 2019-11-06 | <i class="fa fa-folder-o"></i> <a class="category-link" href="/mengxiao2000.github.io/categories/深度学习/">深度学习</a></span>
      <p></p><h1 id="运行TensorFlow"><a href="#运行TensorFlow" class="headerlink" title="运行TensorFlow"></a>运行TensorFlow</h1><h3 id="START"><a href="#START" class="headerlink" title="START"></a>START</h3><ul>
<li>安装：pip安装成功但是不能运行（可能是版本不匹配还有环境不行）<br>用conda安装成功运行。</li>
<li>运行<br>变量初始化；x.initializer.run()等价于tf.get_default_session().run(x.initializer)<br>f.eval等价于tf.get_default_session().run(f)<br>Init=tf.global_variables_initializer()在图中创建一个节点，这个节点在会话执行时初始化所有变量。<br>sess=tf.InteractiveSession()在创建时会把自己设为默认会话。不用with块，但结束后要手动close。<blockquote>
<p>一个TensorFlow程序通常可分为计算图（构建阶段）和执行图（执行阶段）。  </p>
</blockquote>
</li>
<li>管理图<br>管理多个互不依赖的图时，可创建新图并设为默认图<br>Graph = tf.Graph()<br>with graphs.as_default():<br>实验中多次执行同一命令会导致重复节点的添加，可以重启Jupyter内核，或者重置默认图tf.reset_defauly_graph()</li>
<li>节点生命周期<br>图每次执行期间，所有节点值都会被丢弃，但是变量不会，变量由会话维护，由初始化器开始，到关闭结束。</li>
</ul>
<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>闭式解<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=“X”)</span><br><span class="line">y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=“y”)</span><br><span class="line">XT = tf.transpose(X)</span><br><span class="line">theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,X)),XT),y)</span><br><span class="line"></span><br><span class="line">with tf.session() as sess:</span><br><span class="line">		theta = theta.eval()</span><br></pre></td></tr></table></figure></p>
<p>梯度下降<br>    要先对数据归一化，否则过程会很慢。<br>    tf.random_uniform（[,]，,）创建一个节点，生成一个由传入形状和值域随机生成的数填充的张量。<br>    tf.assign(,)创建为变量赋值的节点。<br>自动微分<br>    gradients = tf.gradients(muse, [theta])[0]</p>
<h3 id="使用优化器"><a href="#使用优化器" class="headerlink" title="使用优化器"></a>使用优化器</h3><p>optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning=learninig)<br>动量优化器<br>optimzer = tf.train.MomentumOptimizer(learning_rate=learninig_rate, momentum=0.9)</p>
<h3 id="为算法提供数据"><a href="#为算法提供数据" class="headerlink" title="为算法提供数据"></a>为算法提供数据</h3><p>占位符节点<br>A = tf.placeholder（tf.float32, shape=(None,3), name=“A”）#None表示任意尺寸</p>
<h3 id="保存和恢复模型"><a href="#保存和恢复模型" class="headerlink" title="保存和恢复模型"></a>保存和恢复模型</h3><p>在构造器末尾（所有变量节点都创建后）创建Saver节点。<br>saver = tf.train.Saver()<br>在执行器调用save方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">with tf.Session() as sess:</span><br><span class="line">	......</span><br><span class="line">	save_path = saver.save(sess, “./mymodel_final.ckpt”)</span><br></pre></td></tr></table></figure></p>
<p>恢复模型：<br>在构造器末尾创建Saver节点， 在执行期开始时，不是用init节点初始化变量，而是调用restore方法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">with ......:</span><br><span class="line">	saver.restore(sess, “./mymodel_fianl.ckpt”)</span><br><span class="line">	......</span><br></pre></td></tr></table></figure></p>
<p>保存恢复制定名称：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Beta = tf.train.Saver(&#123;“weights”: theta&#125;)</span><br></pre></td></tr></table></figure></p>
<h3 id="TensorBoard可视化"><a href="#TensorBoard可视化" class="headerlink" title="TensorBoard可视化"></a>TensorBoard可视化</h3><p>为防止Tensorboard将每次状态信息合并，用时间戳的方式命名日志文件夹。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">From datetime import datetime</span><br><span class="line"></span><br><span class="line">Now = datetime.utcnow().strftime(“%Y%m%d%H%M%S”)</span><br><span class="line">Root_logdir = “tf_logs”</span><br><span class="line">Log_dir = “&#123;&#125;/run-&#123;&#125;/”.format(root_logdir, now)</span><br></pre></td></tr></table></figure></p>
<p>在构造期最后：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Mse_summary = tf.summary.scalar(‘MSE’, mse)</span><br><span class="line">#scalar是用于显示accuracy，cross entropy，dropout等标量变化趋势的函数。</span><br><span class="line">File_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())</span><br></pre></td></tr></table></figure></p>
<p>在执行器，训练时定期求值mse_summary节点。<br>通过file_writer写入事件文件中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Summary_str = mse_summary.eval(feed_dict=&#123;X:X_batch, y:y_batch&#125;)</span><br><span class="line">Step = epoch* n_batches + batch_index</span><br><span class="line">File_writer.add_summary(summary_str, step)</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>避免在每一步都记录状态信息，会严重拖慢训练速度。<br>程序结束时关闭FileWriter：<br>file_writer.close()</p>
</blockquote>
<p>启动Tensorboard服务器：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tensorboard —logdir tf_logs/。</span><br></pre></td></tr></table></figure></p>
<p>即可从浏览器浏览</p>
<h3 id="命名作用域"><a href="#命名作用域" class="headerlink" title="命名作用域"></a>命名作用域</h3><p>为了避免处理复杂模型时图变得杂乱，创建命名作用域将节点分组。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">with tf.name_scope(“loss”) as scope:</span><br><span class="line">	error = y_pred - y</span><br><span class="line">Print(error.op.name)</span><br></pre></td></tr></table></figure></p>
<h3 id="模块化"><a href="#模块化" class="headerlink" title="模块化"></a>模块化</h3><p>用函数构建，减少重复，结合命名作用域使用，使图更清晰<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def relu(X):</span><br><span class="line">	with tf.name_scope(“relu”):</span><br><span class="line">		......</span><br></pre></td></tr></table></figure></p>
<h3 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h3><p>在图的不同组件中贡献变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Def relu(X):</span><br><span class="line">	if not hasattr(relu, “threshold”):</span><br><span class="line">		relu.threshold = tf.Variable(0.0, name=“threshold”)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Threshold = getVariable(“threshold”, shape=(), initializer=tf.constant_initializer(0.0))</span><br><span class="line">#创建共享变量，如果已经存在就复用。</span><br><span class="line">#shape=()表示标量</span><br></pre></td></tr></table></figure>
<p>通过<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with tf.variable_scope(“relu”, reuse=True):</span><br></pre></td></tr></table></figure></p>
<p>或<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">with.tf.variable_scope(“relu”) as scope:</span><br><span class="line">	scope.reuse_variables()</span><br></pre></td></tr></table></figure></p>
<p>get_variable()复用变量需要通过设置变量作用域实现，只有通过get_variable()创建的变量才可以复用。<br>get_variable()创建的变量以变量作用域名作为前缀命名。</p>
<p></p>
      <div class="post-button"><a class="btn" href="/mengxiao2000.github.io/2019/11/06/E173C137-23FA-40C0-88AE-1E19BD761AB2/" rel="noopener"><i class="fa fa-angle-double-right fa-fw"></i>Read More</a></div>
    </div>
  </div>
  <div class="lx-entry padding">
    <div>
      <h2 class="title"><a href="/mengxiao2000.github.io/2019/11/06/ACAD1A0B-BE7E-4CD7-B605-D327A304F95F/">TensorFlow正则化</a></h2>
      <span class="lx-post-detail"><i class="fa fa-calendar-o"></i> 2019-11-06 | <i class="fa fa-folder-o"></i> <a class="category-link" href="/mengxiao2000.github.io/categories/深度学习/">深度学习</a></span>
      <p></p><h1 id="通过正则化避免过拟合-Exercise"><a href="#通过正则化避免过拟合-Exercise" class="headerlink" title="通过正则化避免过拟合/Exercise"></a>通过正则化避免过拟合/Exercise</h1><h2 id="提前停止"><a href="#提前停止" class="headerlink" title="提前停止"></a>提前停止</h2><p>当验证集的性能开始下降时停止训练</p>
<h2 id="L1和L2正则化"><a href="#L1和L2正则化" class="headerlink" title="L1和L2正则化"></a>L1和L2正则化</h2><ul>
<li>实现<br>把正则项加在成本函数中<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#只有两层的情况</span><br><span class="line">W1 = tf.get_default_graph().get_tensor_by_name(“hidden1/kernel:0”)</span><br><span class="line">W2 = tf.get_default_graph().get_tensor_by_name(“outputs/kernel:0”)</span><br><span class="line"></span><br><span class="line">#成本函数</span><br><span class="line">with tf.name_scope(“loss”):</span><br><span class="line">	xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)</span><br><span class="line">	base_loss = tf.reduce_mean(xentropy, name=“avg_xentropy”)</span><br><span class="line">	reg_losses = tf.reduce_sum(tf.abs(W1) +tf.reduce_sum(tf.abs(W2)))</span><br><span class="line">	loss = tf.add(base_loss, scale*reglosses, name=“loss”)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>参数实现<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">scale = 0.001</span><br><span class="line">my_dense_layer = partial(</span><br><span class="line">	tf.layers.dense, activation = tf.nn.relu,</span><br><span class="line">	kernel_regularizer = tf.contrib.layers.l1_regularizer(scale)</span><br><span class="line">	#l2_regularizer l1_l2_regularizer</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">hidden1 = my_dense_layer(X, n_hidden1, name=“hidden1”)</span><br><span class="line">#......</span><br><span class="line"></span><br><span class="line">#把正则化损失加到成本函数中</span><br><span class="line">with tf.name_scope(“loss”):</span><br><span class="line">	xentropy = tf.nn.sparse_sftmax_cross_entropy_with_logits(labels=y, logits=logits)</span><br><span class="line">	base_loss = tf.reduce_mean(xetropy,name=“avg_xentropy”)</span><br><span class="line">	reg_losses = tf.get_collection(tf.GraphKeys.REGULARZATION_LOSSES)</span><br><span class="line">	loss = tf.add_n([base_loss] + reg_losses, name=“loss”)</span><br></pre></td></tr></table></figure></p>
<h1 id="Dropout正则化"><a href="#Dropout正则化" class="headerlink" title="Dropout正则化"></a>Dropout正则化</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dropot_rate = 0.3</span><br><span class="line">X_drop = tf.layers.dropout(X, dropout_rate, training=training)</span><br><span class="line">with tf.name_scope(“dnn”):</span><br><span class="line">	hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu, name=“hidden1”)</span><br><span class="line">	hidden_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)</span><br><span class="line">	#训练时training设为正，测试时设为False</span><br><span class="line">	#......</span><br></pre></td></tr></table></figure>
<h1 id="最大范数正则化"><a href="#最大范数正则化" class="headerlink" title="最大范数正则化"></a>最大范数正则化</h1><p>对每一个神经元，传入连接权重w满足其L2范数小于等于最大范数超参数r。<br>降低r会增加正则化树木，减少过拟合，可以帮助缓解梯度爆炸和消失问题。</p>
<ul>
<li>实现<br>每一个步骤计算w的L2范数，如果需要就进行裁剪，使w = w*r/||w||_2.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def max_norm_regularizer(threshold, axes=1, name=“max_norm”, collection=“max_norm”):</span><br><span class="line">	def max_name(weights):</span><br><span class="line">		clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)</span><br><span class="line">		clip_weights = tf.assign(weights, clliped, name=name)</span><br><span class="line">		tf.add_to_collection(collection, clip_wights)</span><br><span class="line">		return None</span><br><span class="line">	return max_norm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">max_norm_reg = max_norm_regularizr(threshold=1.0)</span><br><span class="line"></span><br><span class="line">with tf.name_scope(“dnn”):</span><br><span class="line">	hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_regularizer=max_norm_reg, name=“hidden1”) #像正则化器一样调用</span><br><span class="line">	hidden2 = </span><br><span class="line">	logits = tf.layers.dense(hidden2, n_outputs, name=“outputs”)</span><br><span class="line"></span><br><span class="line">#......</span><br><span class="line"></span><br><span class="line">clip_all_weights = tf.get_collection(“max_norm”)</span><br><span class="line">#执行期</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">	init.run()</span><br><span class="line">	for epoch in rane(n_epochs):</span><br><span class="line">		for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):</span><br><span class="line">			sess.run(training_op, feed_dict=&#123;X:X_batch, y:y_batch&#125;)</span><br><span class="line">			sess.run(clip_all_weights)</span><br><span class="line">		accuracy = </span><br><span class="line">		print()</span><br><span class="line">	save_path =</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="数据扩充"><a href="#数据扩充" class="headerlink" title="数据扩充"></a>数据扩充</h2><h2 id="实用指南"><a href="#实用指南" class="headerlink" title="实用指南"></a>实用指南</h2><p>默认DNN配置<br>Initialization： He initialization<br>Activation function： ELU<br>Normalization： Batch Normalization<br>Regularization： dropout<br>Optimizer： Adam<br>Learning rate scheldule： None</p>
<h1 id="Excercise"><a href="#Excercise" class="headerlink" title="Excercise"></a>Excercise</h1><p></p>
      <div class="post-button"><a class="btn" href="/mengxiao2000.github.io/2019/11/06/ACAD1A0B-BE7E-4CD7-B605-D327A304F95F/" rel="noopener"><i class="fa fa-angle-double-right fa-fw"></i>Read More</a></div>
    </div>
  </div>
  <div class="lx-entry padding">
    <div>
      <h2 class="title"><a href="/mengxiao2000.github.io/2019/11/06/C75D058B-B31B-422C-8DC1-06BA5689E88C/">卷积神经网络结构与理解</a></h2>
      <span class="lx-post-detail"><i class="fa fa-calendar-o"></i> 2019-11-06 | <i class="fa fa-folder-o"></i> <a class="category-link" href="/mengxiao2000.github.io/categories/深度学习/">深度学习</a></span>
      <p></p><h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p>参考：<a href="https://blog.csdn.net/weixin_42451919/article/details/81381294" target="_blank" rel="noopener">https://blog.csdn.net/weixin_42451919/article/details/81381294</a><br>视频：<a href="https://b23.tv/av35087157" target="_blank" rel="noopener">https://b23.tv/av35087157</a><br>输出尺寸计算：<br>output_size = (input_size + 2*padding - kernel_size)/stride + 1<br>过滤器（卷积核）</p>
<h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>最大池化<br>平均池化</p>
<h2 id="CNN架构"><a href="#CNN架构" class="headerlink" title="CNN架构"></a>CNN架构</h2><p>典型架构：<br>输入-&gt;卷积-&gt;池化-&gt;卷积-&gt;池化-&gt;全连接-&gt;全连接-&gt;</p>
<blockquote>
<p>使用卷积核的一个常见错误是为其设置一个太大的值。通常两个3X3的内核堆叠在一起的效果核一个9X9的内核相同，同时减少很多计算量。  </p>
</blockquote>
<h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h3><ul>
<li>层—类型—图—尺寸—内核尺寸—步幅—激活</li>
<li>OUT—全连接—<em>—10–</em>—/—RBF</li>
<li>F6—全连接—<em>—84—</em>—/—tanh</li>
<li>C5—卷积—120–1x1–5x5–1–tanh</li>
<li>S4–平均池化—16–5X5–2x2–2–tanh</li>
<li>C3—卷积—16–10x10–5x5–1–tanh</li>
<li>S2–平均池化—6–14x14–2x2–2–tanh</li>
<li>C1—卷积—6–28x28–5x5–1–tanh</li>
<li>In—输入—1–32x32—\—\—\</li>
</ul>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>参考：<a href="https://blog.csdn.net/luoluonuoyasuolong/article/details/81750190" target="_blank" rel="noopener">https://blog.csdn.net/luoluonuoyasuolong/article/details/81750190</a><br>本地响应归一化：对ReLU激活后输出层的某层，借助与上下n层之间的关系进行归一化。</p>
<h3 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h3><p>多个尺寸卷积核<br>B站：<a href="https://b23.tv/av64359556" target="_blank" rel="noopener">https://b23.tv/av64359556</a></p>
<h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><p>残差学习<br>参考： <a href="https://blog.csdn.net/Dr_destiny/article/details/85253158" target="_blank" rel="noopener">https://blog.csdn.net/Dr_destiny/article/details/85253158</a></p>
<h2 id="误差反向传播"><a href="#误差反向传播" class="headerlink" title="误差反向传播"></a>误差反向传播</h2><p>参考：<a href="https://blog.csdn.net/qq_37951753/article/details/79672615" target="_blank" rel="noopener">https://blog.csdn.net/qq_37951753/article/details/79672615</a></p>
<p></p>
      <div class="post-button"><a class="btn" href="/mengxiao2000.github.io/2019/11/06/C75D058B-B31B-422C-8DC1-06BA5689E88C/" rel="noopener"><i class="fa fa-angle-double-right fa-fw"></i>Read More</a></div>
    </div>
  </div>
  <div class="lx-entry padding">
    <div>
      <h2 class="title"><a href="/mengxiao2000.github.io/2019/11/06/E9763895-C4CC-42DE-A71D-A84551E055ED/">PaddlePaddle数据准备</a></h2>
      <span class="lx-post-detail"><i class="fa fa-calendar-o"></i> 2019-11-06 | <i class="fa fa-folder-o"></i> <a class="category-link" href="/mengxiao2000.github.io/categories/深度学习/">深度学习</a></span>
      <p></p><h1 id="PaddlePaddle数据准备"><a href="#PaddlePaddle数据准备" class="headerlink" title="PaddlePaddle数据准备"></a>PaddlePaddle数据准备</h1><h2 id="概念："><a href="#概念：" class="headerlink" title="概念："></a>概念：</h2><p>Batch：多个样本数据组成的一份训练（预测）数据称为batch。每个batch包含的样本数量称为batch_size<br>Epoch：每次便利全体数据集进行训练（预测）的过程称为一轮epoch<br>数据增强训练神经网络的有效手段，增强方式：<br>Shuffle、随机裁剪、图像反转、光照色彩变换、随机加噪……</p>
<h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><ol>
<li>自定义数据读取reader<br>读取原始训练数据-数据增强-组成batch</li>
<li>使用Paddle fluid API将数据送入网络：<ol>
<li>feed方式</li>
<li>Py_reader接口</li>
</ol>
</li>
<li>执行器训练</li>
</ol>
<h2 id="自定义数据读取reader"><a href="#自定义数据读取reader" class="headerlink" title="自定义数据读取reader"></a>自定义数据读取reader</h2><p>reader是一个python生成器（generator），每次通过yield返回一个样本数据，全体数据集便利完毕reader后退出。<br>示例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">def mnist_reader(image_file, label_file):</span><br><span class="line">    def __reader__():</span><br><span class="line">        with open(image_file) as f:</span><br><span class="line">            f.seek(16)</span><br><span class="line">				images = np.reshape(np.fromfile(f, dtype=&apos;unit8&apos;),[-1, 28, 28])</span><br><span class="line">				images = images / 255.0 * 2.0 - 1.0</span><br><span class="line">        with open(label_file) as f:</span><br><span class="line">				f.seek(8)</span><br><span class="line">				labels = np.fromfile(f, dtype=&apos;unit8&apos;)</span><br><span class="line">        with idx in range(len(labels)):</span><br><span class="line">				yield images[idx, :], labels[idx]</span><br><span class="line">    return __reader__</span><br><span class="line"></span><br><span class="line">#训练调用reader</span><br><span class="line">train_reader = mnist_reader(&apos;train-images.idx3-ubyte&apos;,&apos;train-labels.idx1-ubyte&apos;)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#官方api调用</span><br><span class="line">import paddle</span><br><span class="line">train_reader = paddle.dataset.mnist.train()</span><br></pre></td></tr></table></figure>
<h2 id="定义数据增强逻辑"><a href="#定义数据增强逻辑" class="headerlink" title="定义数据增强逻辑"></a>定义数据增强逻辑</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import paddle</span><br><span class="line">import paddle.fluid as fluid</span><br><span class="line">Import numpy as np</span><br><span class="line"></span><br><span class="line">#左右翻转</span><br><span class="line">def random_flipped_reader(reader):</span><br><span class="line">	def __reader__():</span><br><span class="line">		for image, label in reader():</span><br><span class="line">			#生成闭区间[1,2]上离散均匀分布的整数值</span><br><span class="line">			if np.random.random_integers(2) == 1:</span><br><span class="line">				image = np.fliplr(image)#矩阵左右翻转</span><br><span class="line">			yield image, label</span><br><span class="line">	return __reader__</span><br><span class="line"></span><br><span class="line">#随机加噪</span><br><span class="line">def random_noised_reader(reader):</span><br><span class="line">	def __reader__():</span><br><span class="line">		for image, label in reader():</span><br><span class="line">			image += np.random.normal(0, 0.01, size=image.shape)</span><br><span class="line">		yield image</span><br><span class="line">	return __reader__</span><br><span class="line"></span><br><span class="line">train_reader = paddle.dataset.mnist.train()</span><br><span class="line">train_reader = random_noised_reader(random_flipped_reader(train_reader))</span><br></pre></td></tr></table></figure>
<ul>
<li><p>随机shuffle<br>shuffle_reader = paddle.reader.shuffle(train_reader, buf_size=64)</p>
</li>
<li><p>调用paddle.batch组成一个batch的训练数据<br>batch_reader = paddle_batch(shuffle_reader, batch_size=32) </p>
</li>
</ul>
<h2 id="把数据送入网络训练"><a href="#把数据送入网络训练" class="headerlink" title="把数据送入网络训练"></a>把数据送入网络训练</h2><h3 id="Feed方式"><a href="#Feed方式" class="headerlink" title="Feed方式"></a>Feed方式</h3><p>用户先将reader数据通过data feeder转换为Paddle可识别的Tensor格式数据，传入执行器进行训练。</p>
<ul>
<li><p>步骤</p>
<ol>
<li>准备<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">images = fluid.layers.data(name=&apos;image&apos;,shape=[28,28], dtype=&apos;float32&apos;)</span><br><span class="line">labels = fluid.layers.data(name=&apos;labels&apos;, shape=[1],dtype=&apos;int64&apos;)</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ul>
<pre><code>2. 定义feeder对象
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_feeder = fluid.DataFeeder(feed_list= [images, labels], place=place)</span><br></pre></td></tr></table></figure>
<pre><code>3. 训练网络
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">exe.run(fluid.default_startup_program())</span><br><span class="line">for epoch_id in range(epoch_num):</span><br><span class="line">	for batch_id, batch_data in enumerate(batch_reader()):</span><br><span class="line">		exe.run(feed=data_feeder.feed(data),...)</span><br><span class="line">		#DataFeeder.feed()将用户定义的batch_reader数据转化为paddle可识别的Tensor格式数据</span><br></pre></td></tr></table></figure>
<h3 id="py-reader接口送入训练数据"><a href="#py-reader接口送入训练数据" class="headerlink" title="py_reader接口送入训练数据"></a>py_reader接口送入训练数据</h3><p>Python端开启线程往Paddle backend持续送入训练数据，模型训练线程与Python数据送入线程并行执行。</p>
<ul>
<li>与feed方法不同<br>使用py_reader接口数据读取和模型训练过程是异步进行的，效率更高，适合于高速工业训练场景，而feed方式API更简单，适于入门和调试使用。</li>
<li><p>步骤：</p>
<ol>
<li><p>定义准备py_reader对象</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">py_reader = fluid.layers.py_reader(capacity=8, shapes=[28,28], [1]), dtypes=[&apos;float32&apos; &apos;int64&apos;])</span><br><span class="line">#capacity为数据队列容量，shapes和dtypes对应feed方式下data layer shape和dtype设置</span><br></pre></td></tr></table></figure>
</li>
<li><p>调用read_file读取数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#调用read_file接口从py_reader中读取数据</span><br><span class="line">#read_file返回值对应feed方式下的data_layer</span><br><span class="line">images, labels = fluid.layers.read_file(py_reader)</span><br></pre></td></tr></table></figure>
</li>
<li><p>将用户自定义的reader传入py_reader<br>py_reader.decorate_paddle_reader(batch_reader)</p>
</li>
<li><p>训练网络 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">exe.run(fluid.default_startup_program())</span><br><span class="line">for epoch_id in range(n_epochs):</span><br><span class="line">	py_reader.start()#线程启动</span><br><span class="line">	while True:</span><br><span class="line">		try:</span><br><span class="line">			exe.run(...)</span><br><span class="line">		except fluid.core.EOFException:</span><br><span class="line">			pt_reader.reset() </span><br><span class="line">			#每轮epoch结束后，C++Backend抛出EOF异常</span><br><span class="line">			#用户需捕获该异常并通过py_reader.reset()重置py_reader</span><br><span class="line">			#以启动下一轮训练</span><br><span class="line">			break</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ul>
<p></p>
      <div class="post-button"><a class="btn" href="/mengxiao2000.github.io/2019/11/06/E9763895-C4CC-42DE-A71D-A84551E055ED/" rel="noopener"><i class="fa fa-angle-double-right fa-fw"></i>Read More</a></div>
    </div>
  </div>
  <div class="lx-entry padding">
    <div>
      <h2 class="title"><a href="/mengxiao2000.github.io/2019/11/06/BB1DACC2-E923-488A-A854-3C978234714D/">Paddle Fluid编程入门</a></h2>
      <span class="lx-post-detail"><i class="fa fa-calendar-o"></i> 2019-11-06 | <i class="fa fa-folder-o"></i> <a class="category-link" href="/mengxiao2000.github.io/categories/深度学习/">深度学习</a></span>
      <p></p><h1 id="Paddle-Fluid编程入门"><a href="#Paddle-Fluid编程入门" class="headerlink" title="Paddle Fluid编程入门"></a>Paddle Fluid编程入门</h1><h2 id="Paddle-Fluid的整体架构"><a href="#Paddle-Fluid的整体架构" class="headerlink" title="Paddle Fluid的整体架构"></a>Paddle Fluid的整体架构</h2><ul>
<li>组网模块</li>
<li>模型表达与优化</li>
<li>训练模块</li>
<li>服务器预测</li>
<li>移动端预测<h2 id="Paddle-Fluid的使用基本概念"><a href="#Paddle-Fluid的使用基本概念" class="headerlink" title="Paddle Fluid的使用基本概念"></a>Paddle Fluid的使用基本概念</h2></li>
<li>layer：表示一个独立计算的逻辑，通常包含一个或多个operator，layers.relu表示relu计算，layers.pool2d表示pool操作。<br>  Layer有Variable输入和输出。示例：output=fluid.layers.relu(x)</li>
<li>Variable：表示一个变量，可以是一个张量Tensor。也可以是其他类型。Variable进入Layer计算，然后Layer返回Variable。<br>示例：var = fluid.layers.fill_constant(shape=[1], dtype=’float32’, value=5)</li>
<li>Program：包含Variable定义的多个变量和Layer定义的多个计算。<br>Program从用户角度顺序执行。</li>
<li>Executor：用来执行Program，会一次性执行Program中定义的所有计算，可以通过feed来提供Program输入数据，通过fetch_list来获取输出数据。</li>
<li>Control Flow：paddle支持if-else和while等语法来描述灵活的模型逻辑。<br>示例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#create a if-else block</span><br><span class="line">ifcond = fluid.layers.less_than(x=a, y=b)</span><br><span class="line">ie = fluid.layers.IfElse(ifcond)</span><br><span class="line">with ie.true_block():</span><br><span class="line">    c = ie.input(a)</span><br><span class="line">    c += 1</span><br><span class="line">    ie.output(c)</span><br><span class="line"></span><br><span class="line">#output c</span><br><span class="line">exe.run(fluid.default_main_program(), fetch_list=[c])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#while</span><br><span class="line">a = fluid.layers.fill_constant([2,2], dtype=&apos;float32&apos;, value=1.0)</span><br><span class="line">i = fluid.layers.zeros([1], dtype=&apos;int64&apos;)</span><br><span class="line">until = fluid.layers.fill_constant([1], dtype=&apos;int64&apos;, value=10)</span><br><span class="line">data_arr = fluid.layers.array_write(a, i)#把a写入新的array的位置i中</span><br><span class="line">cond = fluid.layers.less_than(x=i, y=until)</span><br><span class="line"></span><br><span class="line">#当i小于10时进行循环</span><br><span class="line">while_op = fluid.layers.While(cond=cond)</span><br><span class="line">with while_op.block():</span><br><span class="line">    a = fluid.layers.array_read(data_arr, i)</span><br><span class="line">    a = a + 1</span><br><span class="line">    i = fluid.layers.increment(x=i, value=1, in_place=True)#为函数中传入参数x增加valve大小，x元素个数必须为1，in_place默认在原变量进行计算。</span><br><span class="line">    fluid.layers.less_than(x=i, y=until, cond=cond)#参数cond是用于存储结果的变量</span><br><span class="line">    fluid.layers.array_write(a, i, data_arr)</span><br><span class="line"></span><br><span class="line">#读出data_arr最后位置的结果</span><br><span class="line">ret = fluid.layers.array_read(data_arr, i-1)</span><br><span class="line"></span><br><span class="line">exe = </span><br><span class="line">exe.run</span><br></pre></td></tr></table></figure>
<ul>
<li>Place：指定运行设备</li>
<li>CompileProgram： 对Program进行优化，生成一个执行的图<br>示例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()</span><br><span class="line"></span><br><span class="line">exe = fluid.Executor()</span><br><span class="line"></span><br><span class="line">#Run some initialization, for example, parameter</span><br><span class="line">exe.run(fluid.default_startup_program())</span><br><span class="line"></span><br><span class="line">#Compile the program for multi-device execution and various optimizations</span><br><span class="line">binary = CompileProgram(fluid.default_main_program()).with_data_parallel(loss_name=cost.name)</span><br><span class="line">Loss = exe.run(binary, feed=&#123;&#125;, fetch=[cost])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Paddle-Fluid编写手写数字识别模型"><a href="#Paddle-Fluid编写手写数字识别模型" class="headerlink" title="Paddle Fluid编写手写数字识别模型"></a>Paddle Fluid编写手写数字识别模型</h2><p></p>
      <div class="post-button"><a class="btn" href="/mengxiao2000.github.io/2019/11/06/BB1DACC2-E923-488A-A854-3C978234714D/" rel="noopener"><i class="fa fa-angle-double-right fa-fw"></i>Read More</a></div>
    </div>
  </div>
  <div class="lx-entry padding">
    <div>
      <h2 class="title"><a href="/mengxiao2000.github.io/2019/11/06/14B6E235-0D5B-4208-955E-CB536F9EB002/">使用Fluid进行单机训练</a></h2>
      <span class="lx-post-detail"><i class="fa fa-calendar-o"></i> 2019-11-06 | <i class="fa fa-folder-o"></i> <a class="category-link" href="/mengxiao2000.github.io/categories/深度学习/">深度学习</a></span>
      <p></p><h1 id="使用Fluid进行单机训练"><a href="#使用Fluid进行单机训练" class="headerlink" title="使用Fluid进行单机训练"></a>使用Fluid进行单机训练</h1><h2 id="神经网络简介"><a href="#神经网络简介" class="headerlink" title="神经网络简介"></a>神经网络简介</h2><ul>
<li>网络结构</li>
<li>模型参数<h2 id="配置模型结构"><a href="#配置模型结构" class="headerlink" title="配置模型结构"></a>配置模型结构</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import paddle</span><br><span class="line">Import paddle.fluid as fluid</span><br><span class="line"></span><br><span class="line">#input层</span><br><span class="line"></span><br><span class="line">image = fluid.layers.data(name=&apos;pixel&apos;, shape=[1,28,28], dtype=&apos;float32&apos;)#(channels, width, height)</span><br><span class="line">label = fluid.layers.data(name=&apos;label&apos;, shape=[1], dtype=&apos;int64&apos;)</span><br><span class="line"></span><br><span class="line">#model</span><br><span class="line">conv1 = fluid.layers.conv2d(input=image, filter_size=5, num_filters=20)</span><br><span class="line">relu1 = fluid.layers.relu(conv1)</span><br><span class="line">pool1 = fluid.layers.pool2d(input=relu1, pool_size=2, pool_stride=2)</span><br><span class="line">conv2 = fluid.layers.conv2d(input=pool1, filter_size=5, filter_num=50)</span><br><span class="line">relu2 = fluid.layers.relu(conv2)</span><br><span class="line">pool2 = fluid.layers.pool2d(input=relu2, pool_size=2, pool_stride=2)</span><br><span class="line"></span><br><span class="line">predict = fluid.layers.fc(input=pool2, size=10, act=&apos;softmax&apos;)</span><br><span class="line"></span><br><span class="line">#loss</span><br><span class="line">cost = fluid.layers.cross_entropy(input=predict, label=label)</span><br><span class="line">avg_cost = fluid.layers.mean(cost)</span><br><span class="line">batch_acc = fluid.layers.accuracy(input=predict, label=label)</span><br><span class="line">#optimizer</span><br><span class="line">opt = fluid.optimizer.AdamOptimizer()</span><br><span class="line">opt.minimize(avg_cost)</span><br><span class="line">#initialize</span><br><span class="line">place = fluid.CPUPlace()</span><br><span class="line">exe = fluid.Executor(place)</span><br><span class="line">#模型训练之前要对参数进行初始化，且只需执行一次初始化操作</span><br><span class="line">#stratup_program存储模型参数的初始化操作</span><br><span class="line">#main_program存储模型网络结构</span><br><span class="line">exe.run(fluid.default_startup_program())</span><br><span class="line"></span><br><span class="line">#初始化后的参数存放在fluid.global_scope()中，可通过参数名从该scope中获取参数</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="单卡训练"><a href="#单卡训练" class="headerlink" title="单卡训练"></a>单卡训练</h2><p>单卡训练可以使用fluid.Executor()中的run方法，运行fluid.Program即可；<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">train_reader = paddle.batch(paddle.dataset.mnist.train(), batch_size=128)</span><br><span class="line"></span><br><span class="line">for epoch_id in range(5):</span><br><span class="line">	for batch_id, data in enumerate(train_reader()):</span><br><span class="line">		img_data = np.array([x[0].reshape([1,28,28]) for x in data]).astype(&apos;float64&apos;)</span><br><span class="line">		y_data = np.array([x[1] for x in data]).reshape([len(img_data),1]).astype(&apos;int64&apos;)</span><br><span class="line">		loss, acc = exe.run(fluid.default_main_program(), feed=&#123;&apos;pixel&apos;:img_data, &apos;label&apos;:y_data&#125;, fetch=[avg_cost, batch_acc])</span><br><span class="line">		print(&quot;epoch:%d, batch:%d, loss:%.5f, acc:%.5f&quot;%(epoch_id, batch_id, loss, acc))</span><br><span class="line"></span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">## 多卡训练</span><br><span class="line">数据并行，将数据分为n份放在不同的卡上训练，最后将结果汇总。</span><br></pre></td></tr></table></figure></p>
<p>from paddle.fluid import compiler</p>
<p>#将构建的Program转换为数据并行模式的Program<br>compiled_program = compiler.CompiledProgram(fluid.default_main_program())<br>compiled_program.with_data_parallel(loss_name=avg_cost.name)</p>
<p>#训练<br>…<br>exe.run(compiled_program(),…)<br>…</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 对于CPU训练，通过设置环境变量 export CPU_NUM=4 指定模型用多个线程训练</span><br><span class="line">* 一个程序包含多个模型，切换Program</span><br></pre></td></tr></table></figure>
<p>#Define Program1<br>main_program_1 = fluid.Program()<br>startup_program_1 = fluid.Program()</p>
<p>with fluid.program_guard(main_program_1, startup_program_1):<br>    im_data_1, label_1, loss_1 = model1()</p>
<p>exe.run(startup_program_1)</p>
<p>for batch_id, data in enumerate(train.reader1()):<br>    img_data, y_data = …<br>    loss = exe.run(main_program_1, feed={in_data_1.name: img.data, ‘label’:y_data}, fetch_list=[loss_1])<br>    print(“…”)</p>
<p>#Define Program2<br>…<br><code>`</code></p>
<ul>
<li>多个Program之间共享参数<br>Paddle采用变量名区分不同变量，且变量名是根据unique_name模块中的计数器自动生成的，每生成一个变量名计数值加1.<br>fluid.unique_name.guard()的作用是重置unique_name模块中的计数器，保证多次调用fluid.unique_name.guard()配置网络时对应变量的变量名相同，从而实现参数共享。</li>
</ul>
<p></p>
      <div class="post-button"><a class="btn" href="/mengxiao2000.github.io/2019/11/06/14B6E235-0D5B-4208-955E-CB536F9EB002/" rel="noopener"><i class="fa fa-angle-double-right fa-fw"></i>Read More</a></div>
    </div>
  </div>
  <div class="lx-entry padding">
    <div>
      <h2 class="title"><a href="/mengxiao2000.github.io/2019/11/06/C3F98461-7799-43E5-8E33-E504E7EDD557/">使用Fluid进行多机训练</a></h2>
      <span class="lx-post-detail"><i class="fa fa-calendar-o"></i> 2019-11-06 | <i class="fa fa-folder-o"></i> <a class="category-link" href="/mengxiao2000.github.io/categories/深度学习/">深度学习</a></span>
      <p></p><h1 id="使用Fluid进行多机训练"><a href="#使用Fluid进行多机训练" class="headerlink" title="使用Fluid进行多机训练"></a>使用Fluid进行多机训练</h1><h2 id="分布式训练"><a href="#分布式训练" class="headerlink" title="分布式训练"></a>分布式训练</h2><p>两种模式：模型并行、数据并行</p>
<ul>
<li>模型并行：<br>分布式系统中的不同机器或设备（CPU/GPU）负责网络的不同部分，计算和参数都可能分配在不同的节点上。</li>
<li><p>数据并行：<br>分布式系统中的不同机器都有一份完整的模型副本，每台机器分配到不同的数据，将所有机器的运算结果按照某种方式合并。</p>
</li>
<li><p>paddle目前主要支持数据并行的训练方式。<br>paddle数据并行的两种模式：</p>
<ul>
<li>parameter server模式<br>有多个pserver进程和多个trainer进程<br>每个pserver进程会保存一部分的模型参数，接收一个从trainer发送的梯度并更新这些模型的参数。<br>每个trainer进程会保存一份完整的模型，并使用一部分数据进行训练，然后像pserver发送梯度，最后从pserver拉取更新后的参数。</li>
<li>Colletive模式<br>没有pserver进程，每个trainer进程都保存一份完整的模型参数，完成梯度计算之后通过trainer之间的相互通讯，Reduce梯度数据到所有节点，然后每个节点再各自完成梯度更新。<br>更适合模型体积较大，需要使用同步训练和GPU训练。</li>
</ul>
</li>
<li><p>Collective（NCCL2）模式注意事项：<br>要确保每个节点训练等量的数据，防止最后一轮训练中任务不退出。通常有两种方式：</p>
<ol>
<li>随机采样一些数据，补全分配到较少数据的节点上。（推荐）</li>
<li>在python代码中，每个节点每个pass只训练固定的batch数，如果这个节点数据较多，则不训练这些多出来的数据。<br>如果系统中有多个网络设备，需要手动指定NCCL2使用的设备，假设需要使用eth2为通讯设备，需要设定如下环境变量：<br>export NCCL_SOCKET_IFNAME=eth2</li>
</ol>
</li>
</ul>
<h2 id="分布式程序转换器"><a href="#分布式程序转换器" class="headerlink" title="分布式程序转换器"></a>分布式程序转换器</h2><p>Paddle Fluid提供一个分布式程序转换器（DistributeTranspiler）来将单机网络转换为分布式训练网络。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#定义单机网络</span><br><span class="line">model()</span><br><span class="line">#配置分布式程序转换器</span><br><span class="line">t = fluid.DitributeTranspiler()</span><br><span class="line">#生成计算图</span><br><span class="line">t.transpile(</span><br><span class="line">	trainer_id = trainer_id,</span><br><span class="line">	preserver = pserver_endpointers,</span><br><span class="line">	trainers = trainers</span><br><span class="line">)</span><br><span class="line">#根据自己的角色启动parameter_server</span><br><span class="line">#pserver 计算图</span><br><span class="line">if training_role == &quot;PSERVER&quot;:</span><br><span class="line">	pserver_prog = t.get_pserver_program(current_endpoint)</span><br><span class="line">	startup_prog = tf.get_startup_program(current_endpoint, pserver_prog)</span><br><span class="line">	exe.run(start_prog)</span><br><span class="line">	exe.run(pserver_prog)</span><br><span class="line"></span><br><span class="line">#根据自己的角色启动trainer</span><br><span class="line">#trainer 计算图</span><br><span class="line">else training_role &quot;TRAINER&quot;:</span><br><span class="line">	trainer_prog = t.get_trainer_program()</span><br><span class="line">	exe.run(fluid.default_startup_program())</span><br><span class="line">	train_loop()</span><br></pre></td></tr></table></figure></p>
<p>配置示例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">role = &quot;PSERVER&quot;</span><br><span class="line">trainer_id = 0</span><br><span class="line">Pserver_endpoints = &quot;192.168.1.1:6170, 192.168.1.2:6170&quot;</span><br><span class="line">Current_endpoint = &quot;192.168.1.1:6170&quot;</span><br><span class="line">trainers = 4</span><br></pre></td></tr></table></figure></p>
<p>具体参数和配置参考官方文档。</p>
<p></p>
      <div class="post-button"><a class="btn" href="/mengxiao2000.github.io/2019/11/06/C3F98461-7799-43E5-8E33-E504E7EDD557/" rel="noopener"><i class="fa fa-angle-double-right fa-fw"></i>Read More</a></div>
    </div>
  </div>
  <div class="lx-entry padding">
    <div>
      <h2 class="title"><a href="/mengxiao2000.github.io/2019/11/06/EDCBE4BC-53BF-4824-A794-8849EF9D4DBD/">PaddleFluid保存模型与恢复训练</a></h2>
      <span class="lx-post-detail"><i class="fa fa-calendar-o"></i> 2019-11-06 | <i class="fa fa-folder-o"></i> <a class="category-link" href="/mengxiao2000.github.io/categories/深度学习/">深度学习</a></span>
      <p></p><h1 id="Fluid保存模型与恢复训练"><a href="#Fluid保存模型与恢复训练" class="headerlink" title="Fluid保存模型与恢复训练"></a>Fluid保存模型与恢复训练</h1><h2 id="保存和加载的模型变量"><a href="#保存和加载的模型变量" class="headerlink" title="保存和加载的模型变量"></a>保存和加载的模型变量</h2><p>保存与加载的内容：模型参数、模型结构、优化器的中间状态等长期变量。<br>长期变量：持续存在，不因一个迭代的结束而销毁；<br>临时变量：每个迭代开始被创建，结束时被销毁。</p>
<h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><p>用于预测：保存符合预测需求的裁剪后的模型及模型参数<br>Fluid.io.load_inference_model<br>Fluid.io.save_inference_model<br>用于恢复训练、增量训练：只会保存长期变量<br>Fluid.io.load_persistables<br>Fluid.io.save_persistables</p>
<h2 id="预测的一般步骤"><a href="#预测的一般步骤" class="headerlink" title="预测的一般步骤"></a>预测的一般步骤</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model_dir = &apos;output/models&apos;</span><br><span class="line">#在每个epoch结束后执行一次save_inference_model保存需要预测的模型及参数</span><br><span class="line">fluid.io.save_inference_model(model_dir, [], [predict, auc_var, cur_auc_var], exe)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#预测</span><br><span class="line">inference_scope = fluid.core.Scope()</span><br><span class="line">with fluid.scope_guard(inference_scope):</span><br><span class="line">	[inference_program, feed_target_names, fetch_targets] = (fluid.io.load_inference_model(save_dirname, exe))</span><br><span class="line">	test_reader =</span><br><span class="line">	results = exe.run(inference_program, feed=...)</span><br></pre></td></tr></table></figure>
<h2 id="单机（一个节点）训练的增量与恢复训练的步骤"><a href="#单机（一个节点）训练的增量与恢复训练的步骤" class="headerlink" title="单机（一个节点）训练的增量与恢复训练的步骤"></a>单机（一个节点）训练的增量与恢复训练的步骤</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#存储模型变量</span><br><span class="line">fluid.io.save_persistables(model_dir, exe, main_program)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#加载模型变量</span><br><span class="line">exe.run(start_program)</span><br><span class="line">#在start_program执行完后执行</span><br><span class="line">fluid.io.load_persistables(exe, load_persistabledir, main_program=start_program)</span><br><span class="line"></span><br><span class="line">pe = fluid.ParallelExecutor(...)</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">	while True:</span><br><span class="line">		cost_val, label_val = pe.run(fectch_list=[avg_cost.name, label.name])</span><br><span class="line">except fluid.core.EOFException():</span><br><span class="line">	reader.reset()</span><br><span class="line">	fluid.io.save_persistables(exe, &quot;...&quot;, main_program=main_program)</span><br></pre></td></tr></table></figure>
<h2 id="分布式训练的增量与恢复训练和单机的不同点"><a href="#分布式训练的增量与恢复训练和单机的不同点" class="headerlink" title="分布式训练的增量与恢复训练和单机的不同点"></a>分布式训练的增量与恢复训练和单机的不同点</h2><ol>
<li>训练最后调用fluid.io.save_persistables保存长期变量时，不必要所有的trainer都调用这个方法，一般0号trainer来保存</li>
<li>多机增量训练的参数加载在PServer端，trainer端不用加载参数。在PServer全部启动后，trainer会从PServer端同步。</li>
</ol>
<p></p>
      <div class="post-button"><a class="btn" href="/mengxiao2000.github.io/2019/11/06/EDCBE4BC-53BF-4824-A794-8849EF9D4DBD/" rel="noopener"><i class="fa fa-angle-double-right fa-fw"></i>Read More</a></div>
    </div>
  </div>
  <div class="lx-entry padding">
    <div>
      <h2 class="title"><a href="/mengxiao2000.github.io/2019/11/06/8AF97168-CDD0-4E4D-8944-DAA48849A8ED/">PaddlePaddle模型评估与调试</a></h2>
      <span class="lx-post-detail"><i class="fa fa-calendar-o"></i> 2019-11-06 | <i class="fa fa-folder-o"></i> <a class="category-link" href="/mengxiao2000.github.io/categories/深度学习/">深度学习</a></span>
      <p></p><h1 id="PaddlePaddle模型评估与调试"><a href="#PaddlePaddle模型评估与调试" class="headerlink" title="PaddlePaddle模型评估与调试"></a>PaddlePaddle模型评估与调试</h1><h2 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h2><p>损失函数op：<br>fluid.layers.cross_entropy<br>fluid.layers.log_loss<br>Fluid.layers.square_error_cost<br>Fluid.layers.rank_loss<br>Fluid.layers.margin_rank_loss<br>Fluid.layers.nce<br>…<br>评估器op：<br>Fluid.metrics.Accuracy<br>Fluid.metrics.Precision<br>Fluid.metrics.Recakk<br>Fluid.metrics.CompositeMetric<br>Fluid.metrics.AUC<br>…<br>详细见官方文档</p>
<h2 id="VisualDL可视化与分析"><a href="#VisualDL可视化与分析" class="headerlink" title="VisualDL可视化与分析"></a>VisualDL可视化与分析</h2><p>安装：<br>pip install –upgrade visualdl<br>运行一个例子：vdl_create_scratch_log<br>启动：<br>visual –logdir=scratch_log –port=8080<br>访问：127.0.0.1:8080</p>
<p>示例：<br>1.cat test.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line">From visual import LogWriter</span><br><span class="line"></span><br><span class="line">Logdir = &apos;./tmp</span><br><span class="line">Logger = LogWriter(logdir, sync_cycle=10000)</span><br><span class="line"></span><br><span class="line">With logger.mode(&apos;train&apos;):</span><br><span class="line">	scalar0 = logger.scalar(&apos;scalar0&apos;)</span><br><span class="line">For step in range(100):</span><br><span class="line">	scalar0.add_record(step,random.random())</span><br></pre></td></tr></table></figure></p>
<p>2.visualdl –logdir=tmp –port=8080<br>3.访问</p>
<blockquote>
<p>在实际训练中也分为2步：1、定义loggerwriter；2、训练过程中add_record;  </p>
</blockquote>
<h2 id="模型评估：有标签测试集"><a href="#模型评估：有标签测试集" class="headerlink" title="模型评估：有标签测试集"></a>模型评估：有标签测试集</h2><p>1、定义配置<br>2、调用fluid.io.load_persistable加载参数<br>3、定义测试集reader<br>4、执行，统计评估指标</p>
<h2 id="模型推理：无标签新样本"><a href="#模型推理：无标签新样本" class="headerlink" title="模型推理：无标签新样本"></a>模型推理：无标签新样本</h2><p>方式1：load_persistable,需定义网络<br>方式2：load_inference_model,无需定义网络</p>
<p></p>
      <div class="post-button"><a class="btn" href="/mengxiao2000.github.io/2019/11/06/8AF97168-CDD0-4E4D-8944-DAA48849A8ED/" rel="noopener"><i class="fa fa-angle-double-right fa-fw"></i>Read More</a></div>
    </div>
  </div>
  <div class="lx-entry padding">
    <div>
      <h2 class="title"><a href="/mengxiao2000.github.io/2019/11/06/6FDAF392-4A0B-41AC-A795-CD2FE44E7812/">强化学习</a></h2>
      <span class="lx-post-detail"><i class="fa fa-calendar-o"></i> 2019-11-06 | <i class="fa fa-folder-o"></i> <a class="category-link" href="/mengxiao2000.github.io/categories/机器学习/">机器学习</a></span>
      <p></p><h1 id="强化学习组成部分"><a href="#强化学习组成部分" class="headerlink" title="强化学习组成部分"></a>强化学习组成部分</h1><p>策略：定义agent基于当前状态的行动<br>值函数valve function：定义我们需要采取用来选择最合适行动的未来奖励的预测<br>模型：预测环境的改变</p>
<h1 id="学习和规划"><a href="#学习和规划" class="headerlink" title="学习和规划"></a>学习和规划</h1><p>与未知环境交互/已知环境模拟</p>
<h3 id="探索和开发"><a href="#探索和开发" class="headerlink" title="探索和开发"></a>探索和开发</h3><p>应对未见情况</p>
<h2 id="马尔可夫决策过程MDP是强化学习核心"><a href="#马尔可夫决策过程MDP是强化学习核心" class="headerlink" title="马尔可夫决策过程MDP是强化学习核心"></a>马尔可夫决策过程MDP是强化学习核心</h2><p>MDP（S，A，P，R，y）<br>回报G_t  从t开始<br>策略： 给定状态下行动的分布<br>值函数v 状态值函数 给定状态 策略派行动的期望回报<br>行动值函数 给定状态s 采取行动a后遵照策略派行动的期望回报</p>
<h2 id="贝尔曼期望方程"><a href="#贝尔曼期望方程" class="headerlink" title="贝尔曼期望方程"></a>贝尔曼期望方程</h2><p>状态值函数分解为<br>行动值函数分解<br>最优状态-值函数 最优行动-值函数</p>
<p>最优方程求解<br>Value Iteration<br>Policy Iteration<br>Q-learning<br>Sarsa</p>
<h2 id="Value-based"><a href="#Value-based" class="headerlink" title="Value-based"></a>Value-based</h2><h2 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h2><p>随机梯度下降最小化MSE</p>
<h2 id="Policy-based"><a href="#Policy-based" class="headerlink" title="Policy-based"></a>Policy-based</h2><p>对策略参数化<br>更好的收敛性<br>高维或连续空间很有效<br>可学习随机策略<br>但是是局部最优收敛</p>
<h2 id="策略目标函数"><a href="#策略目标函数" class="headerlink" title="策略目标函数"></a>策略目标函数</h2><h2 id="Model—based"><a href="#Model—based" class="headerlink" title="Model—based"></a>Model—based</h2><p>监督学习方法学习模型<br>可推理模型的不确定性</p>
<p>学习模型、构造值函数会产生误差</p>
<p></p>
      <div class="post-button"><a class="btn" href="/mengxiao2000.github.io/2019/11/06/6FDAF392-4A0B-41AC-A795-CD2FE44E7812/" rel="noopener"><i class="fa fa-angle-double-right fa-fw"></i>Read More</a></div>
    </div>
  </div>


<div id="pagination">
  <a class="extend prev" rel="prev" href="/mengxiao2000.github.io/page/2/">Prev</a><a class="page-number" href="/mengxiao2000.github.io/">1</a><a class="page-number" href="/mengxiao2000.github.io/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/mengxiao2000.github.io/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/mengxiao2000.github.io/page/6/">6</a><a class="extend next" rel="next" href="/mengxiao2000.github.io/page/4/">Next</a>
</div>

<footer>
  <div>
  Copyright &copy; 2021.<a href="/mengxiao2000.github.io/">黎明已经不远</a><br>Powered by <a href="https://hexo.io" target="_blank">Hexo</a> | Theme <a href="https://lx.js.org" target="_blank">Lx</a><br>
  </div>
</footer>

  </div>
</div>
</div>

<button class="hamburger hamburger--arrow-r" type="button">
    <div class="hamburger-box">
      <div class="hamburger-inner"></div>
    </div>
</button> 
<div class="menu visibility">
  <div class="menu-head">
    <span class="layer">
      <div class="col">
        <div class="row for-pic">
          <div class="profile-pic">
            <a href="/mengxiao2000.github.io/"><img src="/mengxiao2000.github.io/images/avatar.jpeg" alt="Syiao"></a>
          </div>
        </div>
        <div class="row for-name">
          <p>Syiao</p>
          <span class="tagline">Hello, World!</span>
        </div>
      </div>
    </span>
  </div>
  <nav class="menu-container">
  <ul class="menu-items">
    <li><a href="/"><i class="fa fa-home fa-fw"></i>Home</a></li>
    <li><a href="/mengxiao2000.github.io/archives/"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
    
    <li class="has-sub"><span class="dropdown-heading">
      <i class="fa fa-bookmark fa-fw"></i>Pages</span>
        <ul>
          <li><a href="/mengxiao2000.github.io/guestbook/">Guestbook</a></li>
        <li><a href="/mengxiao2000.github.io/about/">About</a></li>
        </ul>
    </li>
    <li class="has-sub"><span class="dropdown-heading">
      <i class="fa fa-link fa-fw"></i>Friends</span>
        <ul>
          <li> <a href="https://lx.js.org" target="_blank">Theme-Lx</a></li>
        </ul>
    </li>
  </ul>
  </nav>
</div>

<div class="gototop js-top">
  <a href="#" class="js-gotop"><i class="fa fa-arrow-up"></i></a>
</div>
<script src="//cdn.jsdelivr.net/npm/theme-lx@0.3.9/source/js/jquery.easing.min.js"></script>
<script>
(function () {
	"use strict";
	var goToTop = function() {
		$(".js-gotop").on("click", function(event){
			event.preventDefault();
			$("html, body").animate({
				scrollTop: $("html").offset().top
			}, 500, "easeInOutExpo");
			return false;
		});
		$(window).scroll(function(){
			var $win = $(window);
			if ($win.scrollTop() > 200) {
				$(".js-top").addClass("active");
			} else {
				$(".js-top").removeClass("active");
			}
		});
	};
	$(function(){
		goToTop();
	});
}());
</script>



</body>
</html>
